{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b70a76d",
   "metadata": {},
   "source": [
    "# 常见激活函数及其在Numpy中的实现与可视化\n",
    "\n",
    "本笔记将介绍深度学习中几种常见的激活函数。对于每种函数，我们将：\n",
    "1.  使用 `Numpy` 实现它。\n",
    "2.  使用 `Matplotlib` 绘制其函数图像和导数图像。\n",
    "3.  在随后的 Markdown 单元格中，分析其优缺点、相关改进以及在不同神经网络中的应用场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义 Sigmoid 函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 定义 Sigmoid 函数的导数\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# 生成数据\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(x)\n",
    "dy = sigmoid_derivative(x)\n",
    "\n",
    "# 绘制图形\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, label='Sigmoid Function')\n",
    "plt.plot(x, dy, label='Sigmoid Derivative', linestyle='--')\n",
    "plt.title('Sigmoid Function and its Derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('value')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0965c",
   "metadata": {},
   "source": [
    "### Sigmoid 函数\n",
    "\n",
    "**优点:**\n",
    "- **输出范围有限：** 函数输出值在 (0, 1) 之间，可以被解释为概率，这在二分类问题的输出层中非常有用。\n",
    "- **平滑性：** Sigmoid 函数是连续可导的，其平滑的导数有助于梯度下降算法的稳定性。\n",
    "\n",
    "**缺点与改进:**\n",
    "- **梯度消失问题（Vanishing Gradients）：** 当输入值非常大或非常小时，Sigmoid 函数的导数趋近于0。在深度网络中，这会导致反向传播时梯度逐层递减，最终使得靠近输入层的网络层权重更新非常缓慢，甚至停止学习。\n",
    "- **非零中心输出：** Sigmoid 的输出不是以0为中心的（恒为正），这会导致后续网络层的输入是非零均值的，可能会降低梯度下降的收敛速度。\n",
    "- **计算成本：** 指数运算（`np.exp()`）相对于其他激活函数（如ReLU）计算成本更高。\n",
    "\n",
    "**改进方向：**\n",
    "- 为了解决梯度消失和非零中心问题，后续的激活函数如 **Tanh** 和 **ReLU** 被提出。Tanh 将输出中心化到0，而ReLU在正数区间的导数恒为1，有效缓解了梯度消失。\n",
    "\n",
    "**在机器学习网络中的应用:**\n",
    "- **传统应用：** 在早期的多层感知机（MLP）和神经网络中被广泛用作隐藏层激活函数。\n",
    "- **现代应用：** 由于其缺点，现在很少在深度网络的隐藏层中使用。其主要应用场景是**二元分类（Binary Classification）**问题的输出层，用于输出一个介于0和1之间的概率值。例如，在逻辑回归模型中，Sigmoid 是其核心组成部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Tanh 函数\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# 定义 Tanh 函数的导数\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# 生成数据\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = tanh(x)\n",
    "dy = tanh_derivative(x)\n",
    "\n",
    "# 绘制图形\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, label='Tanh Function')\n",
    "plt.plot(x, dy, label='Tanh Derivative', linestyle='--')\n",
    "plt.title('Tanh Function and its Derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('value')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871c419",
   "metadata": {},
   "source": [
    "### Tanh (双曲正切) 函数\n",
    "\n",
    "**优点:**\n",
    "- **零中心输出：** Tanh 函数的输出范围是 (-1, 1)，以0为中心。这使得它通常比 Sigmoid 函数收敛更快，因为下一层的输入是零均值的，梯度更新更为均衡。\n",
    "- **更陡的梯度：** 在0附近，Tanh 的导数比 Sigmoid 更大（最大为1，而Sigmoid为0.25），这有助于在训练初期更快地传递梯度。\n",
    "\n",
    "**缺点与改进:**\n",
    "- **梯度消失问题：** 与 Sigmoid 类似，Tanh 在输入的绝对值很大时也会饱和，导数趋近于0，从而引发梯度消失问题，尽管其梯度消失区域的到来比 Sigmoid 稍晚。\n",
    "\n",
    "**改进方向：**\n",
    "- **ReLU** 及其变体（如 Leaky ReLU, ELU）的出现，旨在从根本上解决饱和激活函数（如 Tanh 和 Sigmoid）的梯度消失问题。\n",
    "\n",
    "**在机器学习网络中的应用:**\n",
    "- **循环神经网络（RNNs）：** Tanh 是传统 RNN（包括 LSTM 和 GRU 单元）中非常常见的激活函数。由于其零中心的特性，它在处理序列数据时能更好地维持信息的流动和梯度的传播。\n",
    "- **早期的多层感知机（MLPs）：** 在 ReLU 流行之前，Tanh 因其优于 Sigmoid 的性能，常被用作隐藏层的激活函数。在现代深度网络中，它在隐藏层的使用已基本被 ReLU 及其变体取代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a04b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 ReLU 函数\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# 定义 ReLU 函数的导数\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# 生成数据\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = relu(x)\n",
    "dy = relu_derivative(x)\n",
    "\n",
    "# 绘制图形\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, label='ReLU Function')\n",
    "plt.plot(x, dy, label='ReLU Derivative', linestyle='--')\n",
    "plt.title('ReLU Function and its Derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('value')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa3411",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit) 函数\n",
    "\n",
    "**优点:**\n",
    "- **解决梯度消失问题：** 在正数区间，ReLU 的导数恒为1，这使得梯度可以无衰减地在网络中传播，极大地缓解了深度网络中的梯度消失问题。\n",
    "- **计算效率高：** ReLU 的计算非常简单（一个 `max(0, x)` 操作），远快于 Sigmoid 和 Tanh 中的指数运算，能显著加速网络的训练和推理。\n",
    "- **引入稀疏性：** 对于负数输入，ReLU 的输出为0，这会使网络中的一部分神经元输出为0，从而形成稀疏表示。这种稀疏性可以提高模型的泛化能力，并可能有助于特征解耦。\n",
    "\n",
    "**缺点与改进:**\n",
    "- **非零中心输出：** 和 Sigmoid 类似，ReLU 的输出也是非零均值的。\n",
    "- **Dying ReLU 问题（神经元死亡）：** 如果一个神经元的输入在训练过程中持续为负，那么该神经元的梯度将永远为0，导致其权重无法更新。这种情况一旦发生，该神经元就“死亡”了，不再对任何数据产生响应。这通常由不当的权重初始化或过大的学习率引起。\n",
    "\n",
    "**改进方向：**\n",
    "- **Leaky ReLU / Parametric ReLU (PReLU)：** 为了解决 Dying ReLU 问题，Leaky ReLU 为负数输入分配了一个很小的正斜率（如0.01），使得负数区间的梯度不再为0。PReLU 更进一步，将这个斜率作为一个可学习的参数。\n",
    "- **ELU (Exponential Linear Unit)：** ELU 在负数区具有平滑的饱和特性，输出均值更接近0，通常能带来更快的收敛和更好的泛化，但计算成本更高。\n",
    "- **GELU / Swish 等：** 更平滑的 ReLU 变体，在一些大型模型（如Transformer）中表现更优。\n",
    "\n",
    "**在机器学习网络中的应用:**\n",
    "- **现代深度学习的默认选择：** ReLU 是目前绝大多数深度神经网络（尤其是**卷积神经网络 CNN** 和**深度前馈网络**）隐藏层的首选激活函数。它的简洁性和有效性使其成为构建深度模型的基石。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Leaky ReLU 函数\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, x * alpha)\n",
    "\n",
    "# 定义 Leaky ReLU 函数的导数\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "# 生成数据\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = leaky_relu(x)\n",
    "dy = leaky_relu_derivative(x)\n",
    "\n",
    "# 绘制图形\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, label='Leaky ReLU Function')\n",
    "plt.plot(x, dy, label='Leaky ReLU Derivative', linestyle='--')\n",
    "plt.title('Leaky ReLU Function and its Derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('value')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a539f",
   "metadata": {},
   "source": [
    "### Leaky ReLU 函数\n",
    "\n",
    "**优点:**\n",
    "- **解决 Dying ReLU 问题：** Leaky ReLU 的核心优点是为负数输入提供了一个小的、非零的梯度（由超参数 `alpha` 控制）。这确保了即使神经元接收到负数输入，其权重也能继续更新，从而避免了神经元“死亡”的问题。\n",
    "- **保留 ReLU 的大部分优点：** 它依然计算高效（只有一个额外的乘法），并且在正数区间的行为与 ReLU 完全相同，有助于快速收敛。\n",
    "\n",
    "**缺点与改进:**\n",
    "- **`alpha` 的选择：** `alpha` 的值需要手动设置，选择一个最优的 `alpha` 可能需要额外的调参工作。通常 `alpha` 被设为一个很小的值，如 0.01。\n",
    "- **性能不一致：** 尽管 Leaky ReLU 在理论上优于 ReLU，但在实践中，其性能提升并不总是显著或稳定的。有时它的表现可能与 ReLU 相当，甚至略差。\n",
    "\n",
    "**改进方向：**\n",
    "- **Parametric ReLU (PReLU)：** PReLU 将 `alpha` 作为一个可学习的参数，让网络在训练过程中自行决定最佳的负斜率。这使得激活函数更具适应性，但增加了模型的参数数量。\n",
    "- **Randomized Leaky ReLU (RReLU)：** 在训练过程中，`alpha` 从一个给定的范围内的均匀分布中随机抽取，在测试时则使用一个固定的平均值。\n",
    "\n",
    "**在机器学习网络中的应用:**\n",
    "- **作为 ReLU 的替代品：** 当怀疑或观察到网络中存在大量的“死亡”神经元时，使用 Leaky ReLU 是一个简单有效的尝试。\n",
    "- **生成对抗网络（GANs）：** Leaky ReLU 在 GAN 的判别器中被广泛使用，因为它有助于提供更稳定的梯度，防止判别器过快地压倒生成器。\n",
    "- **通用深度学习模型：** 在各种类型的深度神经网络中，如果 ReLU 的表现不佳，Leaky ReLU 是一个值得尝试的备选方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Softmax 函数\n",
    "def softmax(x):\n",
    "    # 为数值稳定性，减去最大值\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Softmax 通常用于输出层，其导数是一个雅可比矩阵，比较复杂\n",
    "# 这里我们只绘制函数本身\n",
    "# 假设有一个3维的 logits 向量\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "# 绘制条形图\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(range(len(logits)), probabilities)\n",
    "plt.title('Softmax Probabilities')\n",
    "plt.xlabel('Class Index')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(range(len(logits)), ['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Probabilities:\", probabilities)\n",
    "print(\"Sum of Probabilities:\", np.sum(probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051ad8d",
   "metadata": {},
   "source": [
    "### Softmax 函数\n",
    "\n",
    "**优点:**\n",
    "- **概率分布输出：** Softmax 函数将一个实数向量（logits）转换为一个概率分布。输出向量的每个元素都在 (0, 1) 之间，并且所有元素的和为1。这使得它非常适合用于多分类问题的输出层。\n",
    "- **可解释性：** 输出的概率值可以直观地理解为模型对每个类别的置信度。\n",
    "\n",
    "**缺点与改进:**\n",
    "- **非隐藏层激活函数：** Softmax 的设计目标是产生一个概率分布，因此它几乎只用于网络的最后一层，不适用于隐藏层。\n",
    "- **对数值范围敏感：** 输入的 logits 值如果非常大或非常小，直接计算指数可能会导致数值溢出（`inf`）或下溢（`0`）。因此，在实现时通常会通过减去 logits 中的最大值来保证数值稳定性。\n",
    "- **梯度计算复杂：** Softmax 的导数是一个雅可比矩阵，而不是一个简单的标量函数，这使得它在与其他损失函数（如均方误差）结合时梯度计算较为复杂。\n",
    "\n",
    "**改进方向/最佳实践：**\n",
    "- **与交叉熵损失（Cross-Entropy Loss）结合：** 在实践中，Softmax 函数通常与对数似然损失（即交叉熵损失）一起使用。大多数深度学习框架（如 PyTorch, TensorFlow）提供了将 Softmax 和交叉熵损失合并的单个函数（如 `nn.CrossEntropyLoss`）。这样做不仅计算上更高效，而且数值上更稳定，因为它内部处理了 log-softmax 的计算，避免了直接计算 Softmax 带来的数值问题。\n",
    "\n",
    "**在机器学习网络中的应用:**\n",
    "- **多分类（Multi-class Classification）问题的输出层：** 这是 Softmax 的标准和主要应用场景。例如，在图像分类任务中，如果一个模型需要从10个类别（如数字0-9）中选择一个，其最后一层就会使用 Softmax 来输出10个类别的概率。\n",
    "- **注意力机制（Attention Mechanisms）：** 在 Transformer 等模型中，Softmax 被用来计算注意力权重，将注意力分数转换为一个加权和的概率分布。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
