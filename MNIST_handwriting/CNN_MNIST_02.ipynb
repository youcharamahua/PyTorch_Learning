{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2871f06a",
   "metadata": {},
   "source": [
    "# LeNet-5 手写数字识别 (PyTorch + TensorBoardX)\n",
    "\n",
    "此笔记展示使用经典 LeNet-5 结构在 MNIST 数据集上的训练示例。使用 `tensorboardX` 记录训练指标并演示如何可视化。仅使用 PyTorch、torchvision、numpy 与 matplotlib。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4befbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57c1b1",
   "metadata": {},
   "source": [
    "## 计算公式与关键参数速览（LeNet-5 相关）\n",
    "\n",
    "> 面向初学者：下面给出卷积/池化/全连接层的常用计算公式与关键参数解释，便于理解后续网络结构。\n",
    "\n",
    "- 卷积输出尺寸（单维度，高或宽相同计算）\n",
    "  - 公式：$H_{out} = \\left\\lfloor \\frac{H_{in} + 2P - D\\cdot(K-1) - 1}{S} + 1 \\right\\rfloor$\n",
    "  - 符号：$K$=kernel_size，$S$=stride，$P$=padding，$D$=dilation。\n",
    "- 池化输出尺寸（平均/最大池化同理）\n",
    "  - 公式：$H_{out} = \\left\\lfloor \\frac{H_{in} + 2P - (K-1) - 1}{S} + 1 \\right\\rfloor$\n",
    "- Conv2d 关键参数（PyTorch）\n",
    "  - `in_channels`：输入通道数（灰度图为1，RGB为3）\n",
    "  - `out_channels`：卷积核个数（输出通道数）\n",
    "  - `kernel_size`：卷积核大小（如 5 或 (5,5)）\n",
    "  - `stride`：步幅，默认1\n",
    "  - `padding`：边缘补零，默认0\n",
    "  - `dilation`：空洞率，默认1\n",
    "  - `groups`：分组卷积，默认1（常用全通）\n",
    "  - `bias`：是否带偏置，默认True\n",
    "  - `padding_mode`：'zeros'（默认）、'reflect' 等\n",
    "- AvgPool2d 关键参数\n",
    "  - `kernel_size`、`stride`、`padding`：与卷积同名参数含义一致（一般 stride=kernel_size）\n",
    "  - `ceil_mode`：是否向上取整输出尺寸，默认False\n",
    "  - `count_include_pad`：计算平均时是否计入 padding 的0，默认False\n",
    "- Linear（全连接）关键参数\n",
    "  - `in_features`：输入特征维度\n",
    "  - `out_features`：输出特征维度\n",
    "  - `bias`：是否带偏置\n",
    "- 参数量（Parameter Count）估算\n",
    "  - Conv2d：$\\text{params} = C_{out} \\times \\left(\\frac{C_{in}}{\\text{groups}} \\times K_h \\times K_w\\right) + (\\text{bias? } C_{out}:0)$\n",
    "  - Linear：$\\text{params} = \\text{in\\_features} \\times \\text{out\\_features} + (\\text{bias? } \\text{out\\_features}:0)$\n",
    "- 激活函数 Tanh\n",
    "  - 公式：$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$，输出范围(-1,1)\n",
    "- 交叉熵损失 CrossEntropyLoss（训练单元使用）\n",
    "  - 期望输入未归一化`logits`（shape: [N, C]），标签为类别索引`LongTensor`（shape: [N]）\n",
    "  - 内部等价于 LogSoftmax + NLLLoss\n",
    "- 本网络（输入32×32，因前置Pad(2)）各层输出尺寸速查\n",
    "  - 输入：1×32×32（灰度）\n",
    "  - Conv(5×5, stride=1, pad=0) → 6×28×28\n",
    "  - AvgPool(2×2, stride=2) → 6×14×14\n",
    "  - Conv(5×5, stride=1, pad=0) → 16×10×10\n",
    "  - AvgPool(2×2, stride=2) → 16×5×5\n",
    "  - Conv(5×5, stride=1, pad=0) → 120×1×1\n",
    "  - Flatten → 120 → Linear(120→84) → Linear(84→10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9892b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Tanh()\n",
      "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (6): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (7): Tanh()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义 LeNet-5（基于经典结构的 PyTorch 实现）\n",
    "# 说明：\n",
    "# - LeNet-5 是早期的经典卷积网络结构，适合 MNIST 这类小型手写数字识别任务。\n",
    "# - 本实现严格显式填写每个层的所有关键参数，帮助你理解每个参数的作用。\n",
    "# - 输入尺寸采用 32×32：我们在数据预处理里对 28×28 的 MNIST 图像进行了 Pad(2)，方便复现原论文的维度。\n",
    "# - 张量形状约定为 NCHW：N=批大小，C=通道，H=高度，W=宽度。\n",
    "#\n",
    "# 其中 K=kernel_size, S=stride, P=padding, D=dilation。\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        # 特征提取部分（卷积 + 激活 + 池化）\n",
    "        # 输入假设为 1×32×32（灰度图像 + 预处理Pad(2)）\n",
    "        self.features = nn.Sequential(\n",
    "            # 第1个卷积层：1->6 通道，5×5 卷积核，步幅1，无填充\n",
    "            # 形状变化：1×32×32 -> 6×28×28，因为 (32 - 5 + 2*0)/1 + 1 = 28\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5,\n",
    "                      stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'),\n",
    "            # 非线性激活：Tanh，输出范围(-1,1)\n",
    "            nn.Tanh(),\n",
    "            # 平均池化：2×2，步幅2，无填充\n",
    "            # 形状变化：6×28×28 -> 6×14×14\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0,\n",
    "                         ceil_mode=False, count_include_pad=False),\n",
    "\n",
    "            # 第2个卷积层：6->16 通道，5×5 卷积核，步幅1\n",
    "            # 形状变化：6×14×14 -> 16×10×10，因为 (14 - 5)/1 + 1 = 10\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5,\n",
    "                      stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'),\n",
    "            nn.Tanh(),\n",
    "            # 平均池化：2×2，步幅2\n",
    "            # 形状变化：16×10×10 -> 16×5×5\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0,\n",
    "                         ceil_mode=False, count_include_pad=False),\n",
    "\n",
    "            # 第3个卷积层：16->120 通道，5×5 卷积核，步幅1\n",
    "            # 形状变化：16×5×5 -> 120×1×1，因为 (5 - 5)/1 + 1 = 1\n",
    "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5,\n",
    "                      stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # 分类器部分（全连接层）\n",
    "        # 上一层输出是 120×1×1，即每个样本 120 维特征，因此 in_features=120\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 120 -> 84 的全连接层\n",
    "            # 参数量 = 120*84 + 84(偏置) = 10,164\n",
    "            nn.Linear(in_features=120, out_features=84, bias=True),\n",
    "            nn.Tanh(),\n",
    "            # 84 -> num_classes(10) 的输出层（未加Softmax；训练用 CrossEntropyLoss 会内部处理）\n",
    "            # 参数量 = 84*10 + 10(偏置) = 850\n",
    "            nn.Linear(in_features=84, out_features=num_classes, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 形状：N×1×32×32\n",
    "        x = self.features(x)   # 经过卷积/池化后：N×120×1×1\n",
    "        x = x.view(x.size(0), -1)  # 展平为：N×120（保留批维度N，合并其余维度）\n",
    "        x = self.classifier(x)  # 全连接分类：N×10（每个类别一个logit）\n",
    "        return x\n",
    "\n",
    "# 测试模型构建与结构打印（便于确认各层参数与输出形状）\n",
    "model = LeNet5()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b633475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 60000 Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# 数据准备：MNIST (训练/测试)，使用现有的 ./MNIST 数据并禁用下载；将 28x28 pad 到 32x32 以匹配 LeNet 原始设置\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([\n",
    "    # Pad(2)：在图像四周各加 2 个像素（默认用 0 填充），把 28x28 -> 32x32\n",
    "    # 必须在 ToTensor 之前：因为 Pad 操作作用于 PIL 图像（像素值为 0..255），\n",
    "    # 而 ToTensor 会把像素缩放到 0..1 的 float；如果顺序反了，Pad 会作用于张量，行为不同。\n",
    "    transforms.Pad(2),               # 28 -> 32\n",
    "\n",
    "    # ToTensor：PIL Image -> Tensor，且把像素值从 [0,255] 映射到 [0.0,1.0]\n",
    "    # 输出通道顺序为 C×H×W（这里 C=1），数据类型 float32\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalize：按通道做 (x - mean) / std，常用做法是用训练集的全局 mean/std\n",
    "    # 对 MNIST 使用常见值 mean=0.1307, std=0.3081（单通道，所以以元组形式传入）\n",
    "    # 这样做能让输入分布更接近标准正态，有利于优化器稳定与加速收敛\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# root 指向当前目录 '.'，torchvision 会在 './MNIST' 下查找 raw/processed\n",
    "train_dataset = datasets.MNIST(root='.', train=True, download=False, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='.', train=False, download=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print('Train samples:', len(train_dataset), 'Test samples:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f7b5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机样本索引: 53089\n",
      "原始标签 (label): 2\n",
      "原始图像张量形状: (28, 28), dtype: torch.uint8, 像素范围: [0, 255]\n",
      "原始 28x28 图像 (未 Pad / 未 Normalize)：\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tnwr4avfF3iK10awKLNOSS8hwEUDJPvgDpXqfjj4KaP4U8C3uq2+s3VzqVl5byo4QIyu4T7gG5eTkEsehrxOpbe3nu7hLe2hkmmkO1I41LMx9ABya6o/D7x3o9vFqyaFqdsFG9JYQRInbJCncvXuBXZa1pl14F+D99Frdwz674onhZ4JmJeOKM78nPO4E4P+8BXj1ejfCvVINMTXTaXun2HiGa3SOwvNRmWKGFC3705PG/G3Ax29M16Bb/ETSvAlrNc3vjC+8Xa7IpVraCb/AENCSDlTjHY8jPHYVwHxb1zS/Fl7o/iTTbsF7y1MdxZNIGe1dD0I7A7uD3wTXnFFFFFf/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4AWNgoAtgkS3d//+vK3a7Mv78+fP3zwNhbLJ1T8GSf2SwSPr8/IssyYSqhBEIXrQwMqKKQnm1R1eHcMr+wWosRAlcEs1YsKwRVkMhgrxn/q7AKd31908sLslZ3/6+VEWTlCq8eq2wkJfB596fvxNR5Lj8z/z9+/ff37+nZwHpayhyDAuA4QkKUzDeqAaTZAExdAMYLt5a8YJBcqYQkHfrFrIk71Sec86fGRh4FXhgwgha9u8XSQYG8QagnR+fAu20R0gxMMj+WcHAbnkRaOfLXK6Q63+6kCXZU7YfPQN0zJcVdkDh5s8PkSUZ2E8DdS2JApoNAutRJSGCg4MEAA+TeUfB2AGLAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变换后张量形状: (1, 32, 32) (C,H,W)，dtype: torch.float32\n",
      "变换后张量统计: mean=0.0184, std=1.0299, min=-0.4242, max=2.8215\n",
      "变换后 (Pad + Normalize 反归一化) 32x32 图像：\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAgACABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+ivRNE+CfjLW9H/tKO2trWJozJFHdSlZJR2AUA4J7bsV54QVYgjBHBpKdHI8MqSocOjBlPoRX0F8OviH4z8Wa62p6tNHD4d0i3lmvTDAESQiM4BPUtzuwCBx06V8+yMHldh0JJroPAui2XiDxnpum6jcLBZySFpmJxuVVLFR7tjaMdzXvnhS61K4EotfBGi+HfCcUg86TVIiks0an72COWx3bIz3NY3xI1bSrj4UXH/CAz2kWjpe+VqcdpF5Zwx47DgtjnuCByK+eqfDNLbzxzwSPFLGwdJEYqysOQQR0Iq7qWvaxrG3+1NWvr7Z937VcPLj6biaqxXlzBbT20VxNHBcBRNEjkLJtOV3DocHkZ6VDX//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABCElEQVR4AWNgGJKARbZ0//+/rrjdnv7nz5+/f+4L41JR+xSs4I8MDgU+P/6iKmBCV8jEyMj4ooWREV0czq85ujqEQ/YPTisgCpEUYFgBVmEENxA7g+fM3xXYZaCiXX//xOJTMOvb35eqWBRIFVy9VlDAw+Bz78/fiRjynP5n/v79++/v39OzgPQ1DHmGBcDwB8UBGG9UQyhggTB1Axgu3lrxgkFyphBQ4NYtdAU8U3nOOX1hYOBR4EFIIbNk/36RZGAQbwC64eNToBvskSVBbNk/KxjYLC4C3fAyhzPk+p8udAVsyduPngE68MsKO6BU8+eH6AoY2E4DdS+JBNoDAusxFUAkhi8JAL53eTLy1yBTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "说明: 原始图像为 28x28；经 Pad(2) 后四周补零成为 32x32，再做 ToTensor 与 Normalize。上面第二张为已反归一化后的 32x32。\n"
     ]
    }
   ],
   "source": [
    "# 随机抽取并查看一个训练样本：打印张量信息并用 PIL 显示原始与变换后图像\n",
    "import random\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# 随机索引\n",
    "idx = random.randint(0, len(train_dataset) - 1)\n",
    "\n",
    "# 原始 MNIST 数据（未经过 transforms），来自 train_dataset.data (uint8, 28x28)\n",
    "raw_img28 = train_dataset.data[idx]          # shape: (28,28), dtype: torch.uint8\n",
    "raw_label = int(train_dataset.targets[idx])  # 标签\n",
    "\n",
    "print(f\"随机样本索引: {idx}\")\n",
    "print(f\"原始标签 (label): {raw_label}\")\n",
    "print(f\"原始图像张量形状: {tuple(raw_img28.shape)}, dtype: {raw_img28.dtype}, 像素范围: [{raw_img28.min().item()}, {raw_img28.max().item()}]\")\n",
    "\n",
    "# 使用 PIL 展示原始 28x28 灰度图\n",
    "pil_raw = Image.fromarray(raw_img28.numpy(), mode='L')\n",
    "print(\"原始 28x28 图像 (未 Pad / 未 Normalize)：\")\n",
    "display(pil_raw)\n",
    "\n",
    "# 经 transforms 后的图像（已经 Pad -> ToTensor -> Normalize）\n",
    "trans_img_tensor, trans_label = train_dataset[idx]  # trans_label 应与 raw_label 相同\n",
    "assert trans_label == raw_label\n",
    "\n",
    "print(f\"变换后张量形状: {tuple(trans_img_tensor.shape)} (C,H,W)，dtype: {trans_img_tensor.dtype}\")\n",
    "print(f\"变换后张量统计: mean={trans_img_tensor.mean().item():.4f}, std={trans_img_tensor.std().item():.4f}, min={trans_img_tensor.min().item():.4f}, max={trans_img_tensor.max().item():.4f}\")\n",
    "\n",
    "# 反归一化用于可视化：x = x*std + mean\n",
    "_MEAN, _STD = 0.1307, 0.3081\n",
    "unnorm = trans_img_tensor * _STD + _MEAN  # 回到 [0,1] 近似分布（仍然是 32x32）\n",
    "arr32 = unnorm.squeeze().numpy()          # shape: (32,32)\n",
    "arr32_disp = (arr32 * 255.0).clip(0, 255).astype('uint8')\n",
    "pil_trans = Image.fromarray(arr32_disp, mode='L')\n",
    "print(\"变换后 (Pad + Normalize 反归一化) 32x32 图像：\")\n",
    "display(pil_trans)\n",
    "\n",
    "# 对比说明\n",
    "print(\"说明: 原始图像为 28x28；经 Pad(2) 后四周补零成为 32x32，再做 ToTensor 与 Normalize。上面第二张为已反归一化后的 32x32。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dde0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练与验证函数（增强 TensorBoard 可视化）\n",
    "import itertools\n",
    "\n",
    "def log_sample_images(writer, images, global_step, tag_prefix=\"train\"):\n",
    "    # 仅取前 8 张\n",
    "    grid = make_grid(images[:8], nrow=4, normalize=True, scale_each=True)\n",
    "    writer.add_image(f\"{tag_prefix}/samples\", grid, global_step)\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch, writer, log_interval=100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        global_step = epoch * len(train_loader) + batch_idx\n",
    "        # 记录学习率与 batch loss（更细粒度）\n",
    "        writer.add_scalar('metrics/train/batch_loss', loss.item(), global_step)\n",
    "        writer.add_scalar('metrics/train/lr', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "        # 间隔记录样本图像与卷积核权重直方图\n",
    "        if batch_idx == 0:\n",
    "            log_sample_images(writer, data, global_step, tag_prefix=\"images/train\")\n",
    "        if (batch_idx + 1) % (log_interval * 2) == 0:\n",
    "            writer.add_histogram('weights/conv1', model.features[0].weight.detach().cpu(), global_step)\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            acc = 100. * correct / total\n",
    "            writer.add_scalar('metrics/train/avg_loss', avg_loss, global_step)\n",
    "            writer.add_scalar('metrics/train/avg_accuracy', acc, global_step)\n",
    "            print(f'Epoch: {epoch} [{batch_idx+1}/{len(train_loader)}]  Loss: {avg_loss:.4f}  Acc: {acc:.2f}%')\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "\n",
    "def _confusion_matrix(num_classes, preds, targets):\n",
    "    cm = torch.zeros(num_classes, num_classes, dtype=torch.int64)\n",
    "    for p, t in zip(preds, targets):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "\n",
    "def _plot_confusion_matrix(cm, class_names):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_xticks(range(len(class_names)))\n",
    "    ax.set_yticks(range(len(class_names)))\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_yticklabels(class_names)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        val = cm[i, j].item()\n",
    "        if val > 0:\n",
    "            ax.text(j, i, val, ha='center', va='center', color='black', fontsize=8)\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch, writer):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    mis_images = []\n",
    "    mis_pairs = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss_batch = criterion(output, target).item()\n",
    "            test_loss += loss_batch * data.size(0)\n",
    "            _, predicted = output.max(1)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            all_preds.append(predicted.cpu())\n",
    "            all_targets.append(target.cpu())\n",
    "            # 收集前若干误分类样本\n",
    "            mismask = predicted.ne(target)\n",
    "            if mismask.any() and len(mis_images) < 16:\n",
    "                idxs = torch.where(mismask)[0]\n",
    "                for idx in idxs:\n",
    "                    if len(mis_images) >= 16:\n",
    "                        break\n",
    "                    mis_images.append(data[idx].cpu())\n",
    "                    mis_pairs.append((int(target[idx].item()), int(predicted[idx].item())))\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({acc:.2f}%)\\n')\n",
    "    writer.add_scalar('metrics/test/loss', test_loss, epoch)\n",
    "    writer.add_scalar('metrics/test/accuracy', acc, epoch)\n",
    "\n",
    "    # 记录误分类样本图像\n",
    "    if mis_images:\n",
    "        mis_stack = torch.stack(mis_images)\n",
    "        grid = make_grid(mis_stack, nrow=4, normalize=True, scale_each=True)\n",
    "        writer.add_image('images/test/misclassified', grid, epoch)\n",
    "\n",
    "    # 混淆矩阵\n",
    "    preds_cat = torch.cat(all_preds)\n",
    "    targets_cat = torch.cat(all_targets)\n",
    "    cm = _confusion_matrix(10, preds_cat, targets_cat)\n",
    "    fig_cm = _plot_confusion_matrix(cm, [str(i) for i in range(10)])\n",
    "    writer.add_figure('metrics/test/confusion_matrix', fig_cm, epoch)\n",
    "    plt.close(fig_cm)\n",
    "\n",
    "    return test_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0f73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard URL: http://127.0.0.1:6006/\n"
     ]
    }
   ],
   "source": [
    "# 自动启动 TensorBoard 并打开浏览器（在训练前执行）\n",
    "import os, time, sys, subprocess, webbrowser\n",
    "\n",
    "# 与训练日志目录保持一致\n",
    "tb_logdir = './runs/lenet_mnist'\n",
    "os.makedirs(tb_logdir, exist_ok=True)\n",
    "\n",
    "\n",
    "def _start_tensorboard(logdir: str, preferred_port: int = 6006) -> str:\n",
    "    \"\"\"优先通过 TensorBoard API 启动；失败则尝试子进程启动。返回访问 URL。\"\"\"\n",
    "    try:\n",
    "        from tensorboard import program  # 需要安装 `tensorboard`\n",
    "        tb = program.TensorBoard()\n",
    "        tb.configure(argv=[None, '--logdir', logdir, '--port', str(preferred_port), '--host', '127.0.0.1'])\n",
    "        url = tb.launch()\n",
    "        return url\n",
    "    except Exception:\n",
    "        # 退回到子进程方式：python -m tensorboard ...\n",
    "        try:\n",
    "            subprocess.Popen(\n",
    "                [sys.executable, '-m', 'tensorboard', f'--logdir={logdir}', f'--port={preferred_port}', '--host=127.0.0.1'],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.STDOUT,\n",
    "            )\n",
    "            return f'http://127.0.0.1:{preferred_port}'\n",
    "        except Exception as e2:\n",
    "            print('无法启动 TensorBoard，请先安装: pip install tensorboard')\n",
    "            raise e2\n",
    "\n",
    "\n",
    "def start_tensorboard_with_fallbacks(logdir: str, base_port: int = 6006, tries: int = 3) -> str:\n",
    "    for k in range(tries):\n",
    "        port = base_port + k\n",
    "        try:\n",
    "            url = _start_tensorboard(logdir, preferred_port=port)\n",
    "            return url\n",
    "        except Exception:\n",
    "            continue\n",
    "    # 最后返回默认 URL（不保证可用，仅用于提示）\n",
    "    return f'http://127.0.0.1:{base_port}'\n",
    "\n",
    "\n",
    "# 启动并打开浏览器\n",
    "_tb_url = start_tensorboard_with_fallbacks(tb_logdir, base_port=6006, tries=3)\n",
    "print('TensorBoard URL:', _tb_url)\n",
    "# 稍等片刻，确保服务已就绪\n",
    "time.sleep(2)\n",
    "try:\n",
    "    webbrowser.open(_tb_url)\n",
    "except Exception:\n",
    "    print('请在浏览器中手动打开:', _tb_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c32694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [100/938]  Loss: 1.5812  Acc: 58.28%\n",
      "Epoch: 1 [200/938]  Loss: 0.4602  Acc: 87.55%\n",
      "Epoch: 1 [200/938]  Loss: 0.4602  Acc: 87.55%\n",
      "Epoch: 1 [300/938]  Loss: 0.3239  Acc: 90.64%\n",
      "Epoch: 1 [300/938]  Loss: 0.3239  Acc: 90.64%\n",
      "Epoch: 1 [400/938]  Loss: 0.2804  Acc: 91.34%\n",
      "Epoch: 1 [400/938]  Loss: 0.2804  Acc: 91.34%\n",
      "Epoch: 1 [500/938]  Loss: 0.2416  Acc: 92.77%\n",
      "Epoch: 1 [500/938]  Loss: 0.2416  Acc: 92.77%\n",
      "Epoch: 1 [600/938]  Loss: 0.2069  Acc: 93.95%\n",
      "Epoch: 1 [600/938]  Loss: 0.2069  Acc: 93.95%\n",
      "Epoch: 1 [700/938]  Loss: 0.1772  Acc: 94.66%\n",
      "Epoch: 1 [700/938]  Loss: 0.1772  Acc: 94.66%\n",
      "Epoch: 1 [800/938]  Loss: 0.1581  Acc: 95.17%\n",
      "Epoch: 1 [800/938]  Loss: 0.1581  Acc: 95.17%\n",
      "Epoch: 1 [900/938]  Loss: 0.1387  Acc: 95.92%\n",
      "Epoch: 1 [900/938]  Loss: 0.1387  Acc: 95.92%\n",
      "\n",
      "Test set: Average loss: 0.1162, Accuracy: 9635/10000 (96.35%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1162, Accuracy: 9635/10000 (96.35%)\n",
      "\n",
      "Epoch 1 finished in 13.8s\n",
      "Epoch 1 finished in 13.8s\n",
      "Epoch: 2 [100/938]  Loss: 0.1217  Acc: 96.20%\n",
      "Epoch: 2 [100/938]  Loss: 0.1217  Acc: 96.20%\n",
      "Epoch: 2 [200/938]  Loss: 0.1244  Acc: 96.38%\n",
      "Epoch: 2 [200/938]  Loss: 0.1244  Acc: 96.38%\n",
      "Epoch: 2 [300/938]  Loss: 0.0968  Acc: 97.00%\n",
      "Epoch: 2 [300/938]  Loss: 0.0968  Acc: 97.00%\n",
      "Epoch: 2 [400/938]  Loss: 0.0982  Acc: 97.23%\n",
      "Epoch: 2 [400/938]  Loss: 0.0982  Acc: 97.23%\n",
      "Epoch: 2 [500/938]  Loss: 0.0906  Acc: 97.31%\n",
      "Epoch: 2 [500/938]  Loss: 0.0906  Acc: 97.31%\n",
      "Epoch: 2 [600/938]  Loss: 0.0845  Acc: 97.36%\n",
      "Epoch: 2 [600/938]  Loss: 0.0845  Acc: 97.36%\n",
      "Epoch: 2 [700/938]  Loss: 0.0727  Acc: 97.80%\n",
      "Epoch: 2 [700/938]  Loss: 0.0727  Acc: 97.80%\n",
      "Epoch: 2 [800/938]  Loss: 0.0734  Acc: 97.72%\n",
      "Epoch: 2 [800/938]  Loss: 0.0734  Acc: 97.72%\n",
      "Epoch: 2 [900/938]  Loss: 0.0752  Acc: 97.66%\n",
      "Epoch: 2 [900/938]  Loss: 0.0752  Acc: 97.66%\n",
      "\n",
      "Test set: Average loss: 0.0697, Accuracy: 9779/10000 (97.79%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0697, Accuracy: 9779/10000 (97.79%)\n",
      "\n",
      "Epoch 2 finished in 12.9s\n",
      "Epoch 2 finished in 12.9s\n",
      "Epoch: 3 [100/938]  Loss: 0.0702  Acc: 97.94%\n",
      "Epoch: 3 [100/938]  Loss: 0.0702  Acc: 97.94%\n",
      "Epoch: 3 [200/938]  Loss: 0.0630  Acc: 98.09%\n",
      "Epoch: 3 [200/938]  Loss: 0.0630  Acc: 98.09%\n",
      "Epoch: 3 [300/938]  Loss: 0.0572  Acc: 98.33%\n",
      "Epoch: 3 [300/938]  Loss: 0.0572  Acc: 98.33%\n",
      "Epoch: 3 [400/938]  Loss: 0.0713  Acc: 97.83%\n",
      "Epoch: 3 [400/938]  Loss: 0.0713  Acc: 97.83%\n",
      "Epoch: 3 [500/938]  Loss: 0.0605  Acc: 97.92%\n",
      "Epoch: 3 [500/938]  Loss: 0.0605  Acc: 97.92%\n",
      "Epoch: 3 [600/938]  Loss: 0.0715  Acc: 97.75%\n",
      "Epoch: 3 [600/938]  Loss: 0.0715  Acc: 97.75%\n",
      "Epoch: 3 [700/938]  Loss: 0.0595  Acc: 98.12%\n",
      "Epoch: 3 [700/938]  Loss: 0.0595  Acc: 98.12%\n",
      "Epoch: 3 [800/938]  Loss: 0.0566  Acc: 98.16%\n",
      "Epoch: 3 [800/938]  Loss: 0.0566  Acc: 98.16%\n",
      "Epoch: 3 [900/938]  Loss: 0.0629  Acc: 98.11%\n",
      "Epoch: 3 [900/938]  Loss: 0.0629  Acc: 98.11%\n",
      "\n",
      "Test set: Average loss: 0.0556, Accuracy: 9820/10000 (98.20%)\n",
      "\n",
      "Epoch 3 finished in 12.7s\n",
      "\n",
      "Test set: Average loss: 0.0556, Accuracy: 9820/10000 (98.20%)\n",
      "\n",
      "Epoch 3 finished in 12.7s\n",
      "Epoch: 4 [100/938]  Loss: 0.0516  Acc: 98.42%\n",
      "Epoch: 4 [100/938]  Loss: 0.0516  Acc: 98.42%\n",
      "Epoch: 4 [200/938]  Loss: 0.0585  Acc: 98.25%\n",
      "Epoch: 4 [200/938]  Loss: 0.0585  Acc: 98.25%\n",
      "Epoch: 4 [300/938]  Loss: 0.0495  Acc: 98.50%\n",
      "Epoch: 4 [300/938]  Loss: 0.0495  Acc: 98.50%\n",
      "Epoch: 4 [400/938]  Loss: 0.0535  Acc: 98.28%\n",
      "Epoch: 4 [400/938]  Loss: 0.0535  Acc: 98.28%\n",
      "Epoch: 4 [500/938]  Loss: 0.0458  Acc: 98.62%\n",
      "Epoch: 4 [500/938]  Loss: 0.0458  Acc: 98.62%\n",
      "Epoch: 4 [600/938]  Loss: 0.0514  Acc: 98.50%\n",
      "Epoch: 4 [600/938]  Loss: 0.0514  Acc: 98.50%\n",
      "Epoch: 4 [700/938]  Loss: 0.0486  Acc: 98.45%\n",
      "Epoch: 4 [700/938]  Loss: 0.0486  Acc: 98.45%\n",
      "Epoch: 4 [800/938]  Loss: 0.0465  Acc: 98.45%\n",
      "Epoch: 4 [800/938]  Loss: 0.0465  Acc: 98.45%\n",
      "Epoch: 4 [900/938]  Loss: 0.0474  Acc: 98.48%\n",
      "Epoch: 4 [900/938]  Loss: 0.0474  Acc: 98.48%\n",
      "\n",
      "Test set: Average loss: 0.0408, Accuracy: 9869/10000 (98.69%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0408, Accuracy: 9869/10000 (98.69%)\n",
      "\n",
      "Epoch 4 finished in 12.9s\n",
      "Epoch 4 finished in 12.9s\n",
      "Epoch: 5 [100/938]  Loss: 0.0442  Acc: 98.72%\n",
      "Epoch: 5 [100/938]  Loss: 0.0442  Acc: 98.72%\n",
      "Epoch: 5 [200/938]  Loss: 0.0427  Acc: 98.84%\n",
      "Epoch: 5 [200/938]  Loss: 0.0427  Acc: 98.84%\n",
      "Epoch: 5 [300/938]  Loss: 0.0389  Acc: 98.69%\n",
      "Epoch: 5 [300/938]  Loss: 0.0389  Acc: 98.69%\n",
      "Epoch: 5 [400/938]  Loss: 0.0464  Acc: 98.61%\n",
      "Epoch: 5 [400/938]  Loss: 0.0464  Acc: 98.61%\n",
      "Epoch: 5 [500/938]  Loss: 0.0354  Acc: 99.00%\n",
      "Epoch: 5 [500/938]  Loss: 0.0354  Acc: 99.00%\n",
      "Epoch: 5 [600/938]  Loss: 0.0477  Acc: 98.55%\n",
      "Epoch: 5 [600/938]  Loss: 0.0477  Acc: 98.55%\n",
      "Epoch: 5 [700/938]  Loss: 0.0397  Acc: 98.88%\n",
      "Epoch: 5 [700/938]  Loss: 0.0397  Acc: 98.88%\n",
      "Epoch: 5 [800/938]  Loss: 0.0427  Acc: 98.61%\n",
      "Epoch: 5 [800/938]  Loss: 0.0427  Acc: 98.61%\n",
      "Epoch: 5 [900/938]  Loss: 0.0445  Acc: 98.78%\n",
      "Epoch: 5 [900/938]  Loss: 0.0445  Acc: 98.78%\n",
      "\n",
      "Test set: Average loss: 0.0401, Accuracy: 9872/10000 (98.72%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0401, Accuracy: 9872/10000 (98.72%)\n",
      "\n",
      "Epoch 5 finished in 12.6s\n",
      "Model saved to ./lenet_mnist.pth\n",
      "Epoch 5 finished in 12.6s\n",
      "Model saved to ./lenet_mnist.pth\n"
     ]
    }
   ],
   "source": [
    "# 主运行：模型、优化器和训练循环\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LeNet5().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TensorBoardX writer（写入 runs/lenet_mnist）\n",
    "logdir = './runs/lenet_mnist'\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "start_epoch = 1\n",
    "epochs = 5  # 可以根据需要调整\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    t0 = time.time()\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch, writer)\n",
    "    test(model, device, test_loader, criterion, epoch, writer)\n",
    "    print(f'Epoch {epoch} finished in {time.time() - t0:.1f}s')\n",
    "\n",
    "# 保存模型\n",
    "model_path = './lenet_mnist.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print('Model saved to', model_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e178ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 示例：加载保存的模型并可视化若干预测结果\n",
    "# 加载模型\n",
    "model = LeNet5()\n",
    "model.load_state_dict(torch.load('./lenet_mnist.pth', map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "# 取几张测试图像并预测\n",
    "examples = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        if i >= 1:\n",
    "            break\n",
    "        outputs = model(data)\n",
    "        preds = outputs.argmax(dim=1).numpy()\n",
    "        for j in range(min(8, data.size(0))):\n",
    "            img = data[j].squeeze().numpy()\n",
    "            examples.append(img)\n",
    "            labels.append((int(target[j].item()), int(preds[j].item())))\n",
    "\n",
    "# 绘制\n",
    "plt.figure(figsize=(12, 6))\n",
    "for idx, img in enumerate(examples):\n",
    "    plt.subplot(2, 4, idx+1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    gt, pr = labels[idx]\n",
    "    plt.title(f'GT:{gt} Pred:{pr}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd0fe26",
   "metadata": {},
   "source": [
    "## 本地手写窗口（Tkinter）\n",
    "\n",
    "- 运行下一个单元将打开一个本地窗口。\n",
    "- 操作：按住左键在白板上书写；点击“识别”进行推断；“清空”重置画布；“退出”关闭窗口。\n",
    "- 说明：此窗口使用 Tkinter（Windows 通常自带）。若环境未安装 Tk 支持，可能无法启动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35464471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地手写窗口：Tkinter 画布 + LeNet-5 推理\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Tkinter 可能在某些环境不可用\n",
    "try:\n",
    "    import tkinter as tk\n",
    "except Exception as e:\n",
    "    print(\"未能导入 Tkinter：\", e)\n",
    "    raise\n",
    "\n",
    "# 复用已定义的 LeNet5，如果此单元单独运行则做一次定义\n",
    "try:\n",
    "    LeNet5\n",
    "except NameError:\n",
    "    class LeNet5(nn.Module):\n",
    "        def __init__(self, num_classes=10):\n",
    "            super().__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'),\n",
    "                nn.Tanh(),\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=False),\n",
    "                nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'),\n",
    "                nn.Tanh(),\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=False),\n",
    "                nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(in_features=120, out_features=84, bias=True),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(in_features=84, out_features=num_classes, bias=True)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "# 加载模型（CPU 推理）\n",
    "_device = torch.device('cpu')\n",
    "_model = LeNet5().to(_device)\n",
    "weights_path = './lenet_mnist.pth'\n",
    "if os.path.exists(weights_path):\n",
    "    _model.load_state_dict(torch.load(weights_path, map_location=_device))\n",
    "    _model.eval()\n",
    "else:\n",
    "    print('警告：未找到模型权重 ./lenet_mnist.pth，请先运行训练单元保存模型。')\n",
    "\n",
    "_MEAN, _STD = 0.1307, 0.3081\n",
    "\n",
    "\n",
    "def _preprocess_pil(pil_img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"将 PIL 图像转换为 1×1×32×32 标准化张量（与训练一致）。\"\"\"\n",
    "    if pil_img.mode != 'L':\n",
    "        pil_img = pil_img.convert('L')\n",
    "    pil_img = pil_img.resize((28, 28), Image.NEAREST)\n",
    "    arr = np.array(pil_img).astype(np.float32) / 255.0\n",
    "    arr = 1.0 - arr  # 画布白底黑字 -> MNIST 黑底白字\n",
    "    arr = (arr - _MEAN) / _STD\n",
    "    arr = np.pad(arr, pad_width=((2, 2), (2, 2)), mode='constant', constant_values=0.0)\n",
    "    ten = torch.from_numpy(arr)[None, None, :, :].to(_device)\n",
    "    return ten\n",
    "\n",
    "\n",
    "def _predict_from_pil(pil_img: Image.Image):\n",
    "    if _model is None:\n",
    "        return {str(i): 0.0 for i in range(10)}\n",
    "    x = _preprocess_pil(pil_img)\n",
    "    with torch.no_grad():\n",
    "        logits = _model(x)\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    return {str(i): float(probs[i]) for i in range(10)}\n",
    "\n",
    "\n",
    "# === Tkinter 手写窗口 ===\n",
    "CANVAS_SIZE = 280            # 画布像素大小（放大版）\n",
    "BRUSH_WIDTH = 12             # 笔刷粗细\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title('MNIST 手写数字识别 (LeNet-5) - 本地窗口')\n",
    "\n",
    "canvas = tk.Canvas(root, width=CANVAS_SIZE, height=CANVAS_SIZE, bg='white')\n",
    "canvas.pack(padx=8, pady=8)\n",
    "\n",
    "# 用于推理的灰度图缓存（白底）\n",
    "buffer_img = Image.new('L', (CANVAS_SIZE, CANVAS_SIZE), color=255)\n",
    "buffer_draw = ImageDraw.Draw(buffer_img)\n",
    "\n",
    "last_pos = {'x': None, 'y': None}\n",
    "\n",
    "\n",
    "def on_button_press(event):\n",
    "    last_pos['x'], last_pos['y'] = event.x, event.y\n",
    "\n",
    "\n",
    "def on_move(event):\n",
    "    lx, ly = last_pos['x'], last_pos['y']\n",
    "    if lx is None or ly is None:\n",
    "        last_pos['x'], last_pos['y'] = event.x, event.y\n",
    "        return\n",
    "    x, y = event.x, event.y\n",
    "    canvas.create_line(lx, ly, x, y, width=BRUSH_WIDTH, fill='black', capstyle=tk.ROUND, smooth=True)\n",
    "    buffer_draw.line([lx, ly, x, y], fill=0, width=BRUSH_WIDTH)\n",
    "    last_pos['x'], last_pos['y'] = x, y\n",
    "\n",
    "\n",
    "def on_button_release(event):\n",
    "    last_pos['x'], last_pos['y'] = None, None\n",
    "\n",
    "\n",
    "def clear_canvas():\n",
    "    canvas.delete('all')\n",
    "    buffer_draw.rectangle([(0, 0), (CANVAS_SIZE, CANVAS_SIZE)], fill=255)\n",
    "    result_var.set('结果：')\n",
    "\n",
    "\n",
    "def predict_canvas():\n",
    "    probs = _predict_from_pil(buffer_img)\n",
    "    # 取 Top-3 显示\n",
    "    items = sorted([(int(k), v) for k, v in probs.items()], key=lambda kv: kv[1], reverse=True)[:3]\n",
    "    text = '结果：' + '  '.join([f'{k}: {v*100:.2f}%' for k, v in items])\n",
    "    result_var.set(text)\n",
    "\n",
    "\n",
    "btn_frame = tk.Frame(root)\n",
    "btn_frame.pack(fill='x', padx=8, pady=4)\n",
    "\n",
    "tk.Button(btn_frame, text='识别', command=predict_canvas).pack(side='left', padx=4)\n",
    "tk.Button(btn_frame, text='清空', command=clear_canvas).pack(side='left', padx=4)\n",
    "tk.Button(btn_frame, text='退出', command=root.destroy).pack(side='right', padx=4)\n",
    "\n",
    "result_var = tk.StringVar(value='结果：')\n",
    "result_label = tk.Label(root, textvariable=result_var, anchor='w')\n",
    "result_label.pack(fill='x', padx=8, pady=4)\n",
    "\n",
    "canvas.bind('<ButtonPress-1>', on_button_press)\n",
    "canvas.bind('<B1-Motion>', on_move)\n",
    "canvas.bind('<ButtonRelease-1>', on_button_release)\n",
    "\n",
    "# 启动窗口（注意：在某些 Jupyter 环境中主线程阻塞是预期行为）\n",
    "try:\n",
    "    root.mainloop()\n",
    "except Exception as e:\n",
    "    print('Tkinter 主循环启动失败：', e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
